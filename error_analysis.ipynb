{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from PIL import Image\n",
    "import random\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "from sklearn.metrics import classification_report\n",
    "from scipy.special import softmax\n",
    "import time\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, utils, datasets\n",
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "from albumentations import Resize, Compose\n",
    "from albumentations.pytorch.transforms import ToTensor\n",
    "from albumentations.augmentations.transforms import Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_image(img_path):\n",
    "    # transforamtions for input data\n",
    "    transforms = Compose([\n",
    "        Resize(244,244,interpolation=cv2.INTER_NEAREST),\n",
    "        Normalize([0.485, 0.456, 0.406],\n",
    "                  [0.229, 0.224, 0.225]),\n",
    "        ToTensor(),\n",
    "    ])\n",
    "    input_img = cv2.imread(img_path)\n",
    "    input_img = cv2.cvtColor(input_img, cv2.COLOR_BGR2RGB)\n",
    "    input_data = transforms(image=input_img)['image']\n",
    "    # add batch dimension\n",
    "    batch_data = torch.unsqueeze(input_data,0)\n",
    "    return batch_data\n",
    "\n",
    "def preprocessing_image_sample(img_path):\n",
    "    # transforamtions for input data\n",
    "    transforms = Compose([\n",
    "        Resize(244,244,interpolation=cv2.INTER_NEAREST),\n",
    "        ToTensor(),\n",
    "    ])\n",
    "    input_img = cv2.imread(img_path)\n",
    "    input_img = cv2.cvtColor(input_img, cv2.COLOR_BGR2RGB)\n",
    "    input_data = transforms(image=input_img)['image']\n",
    "    # add batch dimension\n",
    "    batch_data = torch.unsqueeze(input_data,0)\n",
    "    return batch_data\n",
    "def load_data (root_path):\n",
    "#     root_path = './data/nude_sexy_safe_v1_x320/testing/'\n",
    "    classes = ['porn','neutral','sexy']\n",
    "    subfolders = [d for d in os.listdir(root_path) if d in classes]\n",
    "    classes = {c:i for i,c in enumerate(subfolders)}\n",
    "    print('there are three classes: {}'.format(classes))\n",
    "    test_data = {}\n",
    "\n",
    "    for folder in subfolders:\n",
    "        items = os.listdir(os.path.join(root_path,folder))\n",
    "        for names in items:\n",
    "            if names.endswith(\".jpg\") or names.endswith(\".jpeg\") or names.endswith(\".png\"):\n",
    "                label = classes[folder]\n",
    "                test_data[os.path.join(root_path,folder,names)] = label\n",
    "    print('there are {} samples in the testing set'.format(len(test_data)))\n",
    "    return test_data,classes\n",
    "\n",
    "def postprocessing(output):\n",
    "    confidence = torch.nn.functional.softmax(output,dim=1)[0]\n",
    "#     confidence, indices = torch.sort(confidence, descending=True)\n",
    "    return confidence\n",
    "\n",
    "def postprocessing_0(output):\n",
    "    confidence = torch.nn.functional.softmax(output,dim=1)[0]*100\n",
    "    _, indices = torch.sort(output, descending=True)\n",
    "    return indices[0]\n",
    "\n",
    "def compute_accuracy(gts,dts):\n",
    "    accurate = 0\n",
    "    total = 0\n",
    "    for key in gts.keys():\n",
    "        if gts[key] == dts[key]:\n",
    "            accurate += 1\n",
    "        total += 1\n",
    "    return accurate, total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    data_dir = './data/clean_nsfw/'\n",
    "    model = 'resnet50'\n",
    "    device = [0]\n",
    "    batch_size = 8\n",
    "    print_freq = 10\n",
    "    checkpoint = './ckpt/model_26_0.pth'\n",
    "    workers = 1\n",
    "\n",
    "\n",
    "args=Args()\n",
    "classes = torch.load(args.checkpoint)['classes']\n",
    "print(classes)\n",
    "model = torchvision.models.__dict__[args.model](pretrained=False)\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, len(classes))\n",
    "model.cuda()\n",
    "model.load_state_dict(torch.load(args.checkpoint)['model'])\n",
    "model.eval()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_data is save in a dictionary [path]:label\n",
    "test_data,classes = load_data('{}/test/'.format(args.data_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch version evaluation\n",
    "count = 0\n",
    "start_time = time.time()\n",
    "save_sample = {}\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    model.cuda()\n",
    "    test_res = {}\n",
    "    for key in test_data.keys():\n",
    "        count += 1 \n",
    "        image = preprocessing_image(key).cuda()\n",
    "#         sample_image = preprocessing_image_sample(key).cuda()\n",
    "#         sample_image = sample_image.cpu().numpy() *255\n",
    "#         save_sample['inputs'] = sample_image.astype('uint8')\n",
    "        output = model(image)\n",
    "        pred = postprocessing(output)\n",
    "#         save_sample['outputs'] = pred.cpu().numpy()\n",
    "        test_res[key] = pred.cpu().numpy()\n",
    "#         break\n",
    "\n",
    "end_time = time.time()\n",
    "print('RunTime: {}, FPS: {}'.format(end_time-start_time,len(test_data)/(end_time-start_time)))\n",
    "# compute_accuracy(test_data,test_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def per_class_analysis (y_pred, y_true, classes):\n",
    "    TP = [0 for i in range(len(classes))]\n",
    "    FN = [0 for i in range(len(classes))]\n",
    "    FP = [0 for i in range(len(classes))]\n",
    "    precision = [0 for i in range(len(classes))]\n",
    "    recall = [0 for i in range(len(classes))]\n",
    "    y_pred_clean = []\n",
    "    y_true_clean = []\n",
    "    for key in y_pred.keys():\n",
    "        confidence, label_pred = class_score_dict(y_pred[key],classes)\n",
    "        label_true = y_true[key]\n",
    "        y_pred_clean.append(label_pred)\n",
    "        y_true_clean.append(label_true)\n",
    "        if label_true == label_pred:\n",
    "            TP[label_true] += 1\n",
    "        else:\n",
    "            FP[label_pred] += 1\n",
    "            FN[label_true] += 1\n",
    "    \n",
    "    for i in range(len(classes)):\n",
    "        precision[i] = TP[i]/(TP[i]+FP[i]+ 1e-16)\n",
    "        recall[i] = TP[i] / (TP[i] + FN[i]+ 1e-16)\n",
    "    \n",
    "    return precision,recall,y_pred_clean,y_true_clean\n",
    "        \n",
    "        \n",
    "\n",
    "def class_score_dict (y,classes):\n",
    "    y_class = {classes[key]:y[classes[key]] for key in classes.keys()}\n",
    "    \n",
    "    y_largest = np.argmax(y, axis=0)\n",
    "    return y_class,y_largest  \n",
    "\n",
    "def per_class_analysis_threshold (y_pred, y_true, classes,threshold):\n",
    "    TP = [0 for i in range(len(classes)+1)]\n",
    "    FN = [0 for i in range(len(classes)+1)]\n",
    "    FP = [0 for i in range(len(classes)+1)]\n",
    "    precision = [0 for i in range(len(classes)+1)]\n",
    "    recall = [0 for i in range(len(classes)+1)]\n",
    "    y_pred_clean = []\n",
    "    y_true_clean = []\n",
    "    for key in y_pred.keys():\n",
    "        confidence, label_pred = class_score_dict(y_pred[key],classes)\n",
    "#         print(confidence,label_pred)\n",
    "        label_true = y_true[key]\n",
    "\n",
    "\n",
    "        if confidence[label_pred] < threshold:\n",
    "            label_pred = 3\n",
    "            \n",
    "        y_pred_clean.append(label_pred)\n",
    "        y_true_clean.append(label_true)\n",
    "        if label_true == label_pred:\n",
    "            TP[label_true] += 1\n",
    "        else:\n",
    "            FP[label_pred] += 1\n",
    "            FN[label_true] += 1\n",
    "            \n",
    "    \n",
    "    for i in range(len(classes)):\n",
    "        precision[i] = TP[i]/(TP[i]+FP[i]+ 1e-16)\n",
    "        recall[i] = TP[i] / (TP[i] + FN[i]+ 1e-16)\n",
    "    \n",
    "    return precision,recall,y_pred_clean,y_true_clean\n",
    "\n",
    "'''\n",
    "error_matrix[m][n] indicate the number of samples belonging to class=m being recognized as class=n\n",
    "\n",
    "'''\n",
    "def error_analysis (y_true,y_pred,target_names):\n",
    "    size = len(target_names)\n",
    "    error_matrix = np.zeros([size,size],dtype = int)\n",
    "    for i in range(len(y_pred)):\n",
    "        a = y_true[i]\n",
    "        b = y_pred[i]\n",
    "\n",
    "        if a == b:\n",
    "            error_matrix[a][b] += 1\n",
    "        else:\n",
    "            error_matrix[a][b] += 1\n",
    "            \n",
    "                \n",
    "    return error_matrix\n",
    "\n",
    "def threshold_evaluation(y_pred,y_true,classes,thre_start,thre_end):\n",
    "    x = np.arange(thre_start,thre_end,0.01)\n",
    "    y_0 = []\n",
    "    y_1 = []\n",
    "    y_2 = []\n",
    "    for thre in x:\n",
    "        precision,recall,y_pred_clean, y_true_clean = per_class_analysis_threshold(y_pred,y_true,classes,thre)\n",
    "        y_0.append([precision[0],recall[0]])\n",
    "        y_1.append([precision[1],recall[1]])\n",
    "        y_2.append([precision[2],recall[2]])\n",
    "#         target_names = ['neutral', 'porn','sexy','unqualified']\n",
    "#         confusion_matrix = error_analysis(y_true_clean, y_pred_clean, target_names)\n",
    "#         print(precision,recall)\n",
    "#         print(confusion_matrix)\n",
    "    return x,y_0,y_1,y_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_line_thre (x,y):\n",
    "    y_pred = [i[0] for i in y]\n",
    "    y_recall = [i[1] for i in y]\n",
    "    y_f1 = [y_pred[i]*y_recall[i]*2/(y_pred[i]+y_recall[i])for i in range(len(y_pred))]     \n",
    "    plt.plot(x,y_pred)\n",
    "    plt.plot(x,y_recall)\n",
    "    plt.plot(x,y_f1)\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_thre, y_thre_0, y_thre_1, y_thre_2 = threshold_evaluation(test_res,test_data,classes,0.5,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_line_thre(x_thre,y_thre_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# threshold\n",
    "precision, recall, y_pred_clean, y_true_clean = per_class_analysis_threshold(test_res,test_data,classes,0.99)\n",
    "target_names = ['neutral', 'porn','sexy','unqualified']\n",
    "print(classification_report(y_true_clean, y_pred_clean, target_names=target_names))\n",
    "print(error_analysis(y_true_clean, y_pred_clean, target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall, y_pred_clean, y_true_clean = per_class_analysis(test_res,test_data,classes)\n",
    "\n",
    "target_names = ['neutral', 'porn','sexy']\n",
    "print(classification_report(y_true_clean, y_pred_clean, target_names=target_names))\n",
    "print(error_analysis(y_true_clean, y_pred_clean, target_names))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_multi_images (img_list):\n",
    "#     if len(img_list) > 50:\n",
    "#         img_list = random.sample(img_list, 50)\n",
    "#         img_list.sort()\n",
    "    \n",
    "\n",
    "    result = Image.new(\"RGB\", (2000, 4000))\n",
    "\n",
    "    for index, file in enumerate(img_list):\n",
    "    #     print(index)x\n",
    "    #     path = os.path.expanduser(file)\n",
    "        img = Image.open(file)\n",
    "    #     img.thumbnail((400, 400), Image.ANTIALIAS)\n",
    "        img = img.resize((400, 400))\n",
    "        x = index % 5 * 400\n",
    "        y = index // 5 * 400\n",
    "        w, h = img.size\n",
    "        result.paste(img, (x, y, x + w, y + h))  \n",
    "\n",
    "    # DPI, here, has _nothing_ to do with your screen's DPI.\n",
    "    dpi = 80.0\n",
    "    xpixels, ypixels = 2000, 4000\n",
    "\n",
    "    fig = plt.figure(figsize=(ypixels/dpi, xpixels/dpi), dpi=dpi)\n",
    "    plt.imshow(result)\n",
    "def obtain_error_list(y_pred, y_true,indexA,indexB):\n",
    "    error_list = []\n",
    "    for key in y_pred.keys():\n",
    "        confidence, label_pred = class_score_dict(y_pred[key],classes)\n",
    "        label_true = y_true[key]\n",
    "        if label_pred == indexB and label_true == indexA:\n",
    "            error_list.append(key)\n",
    "    return error_list  \n",
    "\n",
    "def filtered_accuracy (y_pred, y_true,indexA,conf_thre):\n",
    "    total = 0\n",
    "    acc = 0\n",
    "    for key in y_pred.keys():\n",
    "        confidence, label_pred = class_score_dict(y_pred[key],classes)\n",
    "        label_true = y_true[key]\n",
    "        if label_pred != indexA:\n",
    "            continue\n",
    "        if confidence[label_pred] > conf_thre:\n",
    "            total += 1\n",
    "            if label_pred == label_true:\n",
    "                acc += 1\n",
    "    print(total,acc)\n",
    "    return acc/(total+1e-16)\n",
    "\n",
    "def obtain_required_list(y_pred, y_true,indexA):\n",
    "    img_list = []\n",
    "    img_conf_list = []\n",
    "    for key in y_pred.keys():\n",
    "        confidence, label_pred = class_score_dict(y_pred[key],classes)\n",
    "        label_true = y_true[key]\n",
    "        if label_true == indexA:\n",
    "            img_list.append(key)\n",
    "            img_conf_list.append(confidence[indexA])\n",
    "            \n",
    "    return img_list, img_conf_list\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_list = obtain_error_list(test_res,test_data,2,1)\n",
    "show_multi_images (error_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_accuracy(test_res,test_data,2,0.5)\n",
    "# 1401 1381\n",
    "# 0.9857244825124911"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_list, img_conf_list = obtain_required_list(test_res,test_data,2)\n",
    "conf_thre = 0.98\n",
    "error_list = [img_list[i] for i,conf in enumerate(img_conf_list) if conf > conf_thre]\n",
    "show_multi_images (error_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Infant  detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#infant \n",
    "class Args:\n",
    "    data_dir = './dataset/'\n",
    "    model = 'resnet50'\n",
    "    device = [0]\n",
    "    batch_size = 8\n",
    "    print_freq = 10\n",
    "    checkpoint = './ckpt/model_22_0.pth'\n",
    "    workers = 1\n",
    "\n",
    "\n",
    "args=Args()\n",
    "# classes = torch.load(args.checkpoint)['classes']\n",
    "# print(classes)\n",
    "model = torchvision.models.__dict__[args.model](pretrained=False)\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, len(classes))\n",
    "model.cuda()\n",
    "model.load_state_dict(torch.load(args.checkpoint)['model'])\n",
    "model.eval()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = './dataset'\n",
    "folder = 'infant'\n",
    "test_data={}\n",
    "items = os.listdir(os.path.join(root_path,folder))\n",
    "for names in items:\n",
    "    if names.endswith(\".jpg\") or names.endswith(\".jpeg\") or names.endswith(\".png\"):\n",
    "                label = 0 #neutral\n",
    "                test_data[os.path.join(root_path,folder,names)] = label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "start_time = time.time()\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    model.cuda()\n",
    "    test_res = {}\n",
    "    for key in test_data.keys():\n",
    "        count += 1 \n",
    "        image = preprocessing_image(key).cuda()\n",
    "        output = model(image)\n",
    "        pred = postprocessing(output)\n",
    "        test_res[key] = pred.cpu().numpy()\n",
    "\n",
    "end_time = time.time()\n",
    "print('RunTime: {}, FPS: {}'.format(end_time-start_time,len(test_data)/(end_time-start_time)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall, y_pred_clean, y_true_clean = per_class_analysis(test_res,test_data,classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall, y_pred_clean, y_true_clean = per_class_analysis(test_res,test_data,classes)\n",
    "print(error_analysis(y_true_clean, y_pred_clean, target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_list = obtain_error_list(test_res,test_data,0,0)\n",
    "show_multi_images (error_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Nudity)",
   "language": "python",
   "name": "nudity"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
